{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training our CBOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_28292\\284123730.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Blunkett in quotes\\n \\n David Blunkett -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benitez issues warning to Gerrard\\n \\n Liverpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brookside creator's Channel 4 bid\\n \\n The cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brown visits slum on Africa trip\\n \\n Chancell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gritty return for Prince of Persia\\n \\n Still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Parker's saxophone heads auction\\n \\n A saxoph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Reliance unit loses Anil Ambani\\n \\n Anil Amba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Wal-Mart fights back at accusers\\n \\n Two big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>MCI shareholder sues to stop bid\\n \\n A shareh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Student 'inequality' exposed\\n \\n Teenagers fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text\n",
       "0    David Blunkett in quotes\\n \\n David Blunkett -...\n",
       "1    Benitez issues warning to Gerrard\\n \\n Liverpo...\n",
       "2    Brookside creator's Channel 4 bid\\n \\n The cre...\n",
       "3    Brown visits slum on Africa trip\\n \\n Chancell...\n",
       "4    Gritty return for Prince of Persia\\n \\n Still ...\n",
       "..                                                 ...\n",
       "495  Parker's saxophone heads auction\\n \\n A saxoph...\n",
       "496  Reliance unit loses Anil Ambani\\n \\n Anil Amba...\n",
       "497  Wal-Mart fights back at accusers\\n \\n Two big ...\n",
       "498  MCI shareholder sues to stop bid\\n \\n A shareh...\n",
       "499  Student 'inequality' exposed\\n \\n Teenagers fr...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.stem import WordNetLemmatizer    # module for lemmatization\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "#Download the stopwords(if not already done)\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        text : string : a string of text\n",
    "    Output :\n",
    "        tokens : list : a list of tokens (strings)\n",
    "    \"\"\"\n",
    "    #Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #Lowercase the tokens, remove ponctutation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    #Stem the tokens\n",
    "    tokens = [stemer.stem(token) for token in tokens]\n",
    "\n",
    "    #Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    #Remove special characters\n",
    "    pattern = r'[^a-zA-Z\\s]'  # Keep alphanumeric characters and whitespaces\n",
    "    tokens = [re.sub(pattern, '', token) for token in tokens]\n",
    "\n",
    "    #Remove single characters\n",
    "    tokens = [token for token in tokens if len(token)>1]\n",
    "\n",
    "\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function to the text column\n",
    "corpus['Preprocessed_text'] = corpus['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(corpus['Preprocessed_text'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CBOW Model\n",
    "Now that we have our vocabulary and corpus, we can train the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import CBOW class\n",
    "from CBOW import CBOW\n",
    "\n",
    "#Create a CBOW model\n",
    "cbow = CBOW(corpus['Preprocessed_text'], vocab,window_size=10,embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2A\\Project\\NLP\\Document classification\\W2V_CBOW\\CBOW.py:79: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, steps_per_epoch=steps_per_epoch, epochs=epochs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3314/3314 [==============================] - 187s 56ms/step - loss: 0.2981 - accuracy: 0.9987\n",
      "Epoch 2/2\n",
      "3314/3314 [==============================] - 190s 57ms/step - loss: 0.0010 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "cbow.train(batch_size=32,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBOW_V10k_E100_W10_Epochs10\\assets\n"
     ]
    }
   ],
   "source": [
    "#Get the model\n",
    "model = cbow.model\n",
    "\n",
    "#Save the model\n",
    "import tensorflow as tf\n",
    "tf.saved_model.save(model, \"CBOW_V10k_E100_W10_Epochs10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError : \"interfvent\" not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Accessing the embedding layer\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the embedding of a word\n",
    "def get_embedding(word):\n",
    "    try :\n",
    "        word_index = cbow.vocab_indexed[word]\n",
    "    except KeyError:\n",
    "        print('KeyError : 'f'\"{word}\" not in the vocabulary.')\n",
    "        return\n",
    "    return embedding_layer.get_weights()[0][word_index]\n",
    "\n",
    "get_embedding('interfvent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decid', array([[0.3937688]], dtype=float32)),\n",
       " ('commonplac', array([[0.38004914]], dtype=float32)),\n",
       " ('lifethreaten', array([[0.3661789]], dtype=float32)),\n",
       " ('kaprano', array([[0.35764784]], dtype=float32)),\n",
       " ('tremor', array([[0.3488307]], dtype=float32))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity between two vectors\n",
    "def cos_sim(vector1, vector2):\n",
    "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "\n",
    "# Get the most similar words to a given word\n",
    "def most_similar(word, vocab, embedding_layer, topn=5):\n",
    "    word_embedding = get_embedding(word)\n",
    "    similarities = []\n",
    "    for w in vocab:\n",
    "        if w != word:\n",
    "            w_embedding = get_embedding(w)\n",
    "            similarity = cos_sim(word_embedding, w_embedding)\n",
    "            similarities.append((w, similarity))\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:topn]\n",
    "\n",
    "most_similar('sad', vocab, embedding_layer, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The words may be not that similar, because the model was trained on a small corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
